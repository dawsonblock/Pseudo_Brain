# Capsule Brain - Mathematical Foundations

**Detailed Mathematical Theory & Derivations**

---

## Table of Contents

1. [Pseudo-Mode Memory Theory](#1-pseudo-mode-memory)
2. [FRNN Workspace Theory](#2-frnn-workspace)
3. [Invariants & Constraints](#3-invariants)
4. [Gradient Flow Analysis](#4-gradients)
5. [Convergence Properties](#5-convergence)

---

## 1. Pseudo-Mode Memory Theory

### 1.1 Probabilistic Interpretation

**Memory as Distribution**: PMM represents memory as a mixture distribution over prototype locations:

```
p(x) = Î£áµ¢ occupancy_i Â· Î´(x - Î¼áµ¢)

where:
  Î£áµ¢ occupancy_i = 1        (probability axiom)
  Î¼áµ¢ âˆˆ â„á´°                   (prototype locations)
```

### 1.2 Reconstruction via Kernel Density

**Soft Assignment**: Instead of hard assignment to nearest prototype, use soft weighting:

```
Î±(x) = softmax(similarity(x, {Î¼áµ¢}))

where similarity can be:
  - Cosine: s(x, Î¼) = (xÂ·Î¼)/(â€–xâ€–Â·â€–Î¼â€–)
  - RBF: s(x, Î¼) = exp(-â€–x-Î¼â€–Â²/2ÏƒÂ²)
  - Dot product: s(x, Î¼) = xÂ·Î¼
```

**Temperature-Scaled Softmax**:
```
Î±áµ¢(x; Ï„) = exp(sáµ¢/Ï„) / Î£â±¼ exp(sâ±¼/Ï„)

Properties:
  - Ï„ â†’ 0: Hard assignment (argmax)
  - Ï„ â†’ âˆ: Uniform distribution
  - Ï„ = 1: Standard softmax
```

### 1.3 Loss Function Derivation

**Reconstruction Error**:
```
L_recon = ğ”¼â‚“[â€–x - xÌ‚â€–Â²]
        = ğ”¼â‚“[â€–x - Î£áµ¢ Î±áµ¢(x)Â·Î¼áµ¢â€–Â²]
```

**Gradient w.r.t. Prototypes**:
```
âˆ‚L_recon/âˆ‚Î¼â±¼ = -ğ”¼â‚“[2Â·Î±â±¼(x)Â·(x - xÌ‚)]
             = -2Â·ğ”¼â‚“[Î±â±¼(x)Â·residual(x)]
```

This is a **Hebbian-like update**: Prototypes move toward inputs they activate on, weighted by the reconstruction error.

**Sparsity Regularization**:
```
L_sparse = ğ”¼â‚“[H(Î±(x))]
         = ğ”¼â‚“[Î£áµ¢ Î±áµ¢(x)Â·log(Î±áµ¢(x))]

Goal: Encourage sparse activations (few prototypes respond to each input)

Gradient:
âˆ‚L_sparse/âˆ‚Î¼â±¼ involves âˆ‚Î±áµ¢/âˆ‚Î¼â±¼ (implicit differentiation through softmax)
```

### 1.4 Occupancy Dynamics

**EMA Update**:
```
occupancy_i^(t+1) = ÏÂ·occupancy_i^(t) + (1-Ï)Â·ğ”¼_batch[Î±áµ¢(x)]

where:
  Ï âˆˆ [0,1]: Memory factor (typical: 0.99)
  ğ”¼_batch[Î±áµ¢(x)]: Average activation over current batch
```

**Normalization** (enforces probability constraint):
```
occupancy_i â† occupancy_i / Î£â±¼ occupancy_j

Ensures: Î£áµ¢ occupancy_i = 1.0 exactly
```

**Interpretation**: 
- High occupancy â†’ Prototype represents frequently-seen region
- Low occupancy â†’ Prototype underutilized (candidate for pruning)

### 1.5 Importance Dynamics

**Relevance Metric**:
```
relevance_i(x) = 1 - cos_sim(x, Î¼áµ¢)
               âˆˆ [0, 2]

High relevance â†’ Poor reconstruction â†’ Need more attention
```

**EMA Update**:
```
Î»áµ¢^(t+1) = ÏÂ·Î»áµ¢^(t) + (1-Ï)Â·(1 - relevance_i(x))

Clamping: Î»áµ¢ â† max(Î»áµ¢, Î»_min) to prevent negatives
```

**Interpretation**:
- High Î» â†’ Good reconstruction quality â†’ Keep prototype
- Low Î» â†’ Poor reconstruction â†’ Candidate for split/removal

### 1.6 Structural Operations (Theory)

**Merge Criterion**:
```
Merge Î¼áµ¢ and Î¼â±¼ if:
  1. cos_sim(Î¼áµ¢, Î¼â±¼) > Î¸_merge    (close in space)
  2. Both active (occupancy > 0)

Merged prototype:
  Î¼_new = (Î»áµ¢Â·Î¼áµ¢ + Î»â±¼Â·Î¼â±¼) / (Î»áµ¢ + Î»â±¼)    [importance-weighted average]
  Î»_new = Î»áµ¢ + Î»â±¼
  occupancy_new = occupancy_i + occupancy_j
```

**Mathematical Justification**: This preserves the "center of mass" in latent space, weighted by importance.

**Split Criterion**:
```
Split Î¼áµ¢ if:
  1. occupancy_i > Î¸_split_high    (overutilized)
  2. Î»áµ¢ < Î¸_split_low              (poor quality)

New prototypes:
  Î¼áµ¢' = Î¼áµ¢ - ÎµÂ·direction
  Î¼â±¼ = Î¼áµ¢ + ÎµÂ·direction
  
  where direction ~ ğ’©(0, I) or learned

  Î»áµ¢' = Î»â±¼ = Î»áµ¢ / 2
  occupancy_i' = occupancy_j = occupancy_i / 2
```

**Intuition**: High occupancy + low quality â†’ Region needs finer resolution.

**Prune Criterion**:
```
Prune Î¼áµ¢ if:
  occupancy_i < Î¸_prune

Action:
  - Set active_mask[i] = False
  - Set occupancy_i = 0
  - Redistribute mass to remaining modes (via normalization)
```

### 1.7 Spectral Parameters

**Temporal Dynamics** (future work):
```
Pseudomode evolution:
  Î¼áµ¢(t) = Î¼áµ¢(0) Â· exp(-Î³áµ¢Â·t) Â· cos(Ï‰áµ¢Â·t + Ï†áµ¢)

where:
  Î³áµ¢: Decay rate (memory fade)
  Ï‰áµ¢: Oscillation frequency (rhythmic recall)
  Ï†áµ¢: Phase offset
```

**Current Implementation**: Parameters stored but not yet used in dynamics.

---

## 2. FRNN Workspace Theory

### 2.1 Discrete State Space

**Finite State Machine with Soft Transitions**:
```
State space: S = {sâ‚, sâ‚‚, ..., sâ‚–}    (K discrete modes)

Soft state: m_t âˆˆ Î”á´·    (probability simplex)
  Î£â‚– m_t[k] = 1
  m_t[k] â‰¥ 0  âˆ€k
```

**Gumbel-Softmax Trick** (for differentiability):
```
Hard mode selection: k* = argmax(logits)    [not differentiable]

Soft relaxation:
  gâ‚– ~ Gumbel(0, 1)
  m_t[k] = exp((logits[k] + gâ‚–)/Ï„) / Î£â±¼ exp((logits[j] + gâ±¼)/Ï„)

Properties:
  - Differentiable
  - Ï„ â†’ 0: Approaches hard selection
  - Allows gradient flow through discrete choice
```

### 2.2 Per-Mode Memory Banks

**Memory Tensor**:
```
M_t âˆˆ â„á´·Ë£á´°

M_t[k, :] = memory vector for mode k

Reading:
  h_t = Î£â‚– m_t[k] Â· M_t[k, :]    [weighted sum]
  
  Special cases:
    - If m_t = one_hot(k*): h_t = M_t[k*, :]    (hard selection)
    - If m_t = uniform: h_t = (1/K)Â·Î£â‚– M_t[k, :]    (average all)
```

### 2.3 Memory Update Dynamics

**Delta Computation**:
```
Î”m_t = f_memory([x_t, h_t])

where f_memory is an MLP:
  f: â„^(input_dim + memory_dim) â†’ â„^(memory_dim)
```

**Selective Write Gating**:
```
gate_t = Ïƒ(g([x_t]))    âˆˆ [0, 1]

Î”m_t â† gate_t Â· Î”m_t

Purpose: Don't update memory on every step (learn when to write)
```

**EMA Update** (per mode):
```
M_{t+1}[k, :] = ÏÂ·M_t[k, :] + (1-Ï)Â·m_t[k]Â·Î”m_t

Interpretation:
  - Mode k updates proportional to its activation m_t[k]
  - Inactive modes (m_t[k] â‰ˆ 0) barely update
  - EMA prevents sudden memory changes
```

### 2.4 Stickiness (Temporal Coherence)

**Motivation**: Prevent rapid mode switching.

**Update Rule**:
```
m_t^(raw) = gumbel_softmax(MLP(x_t))    [initial selection]

m_t = (1-Î²)Â·m_t^(raw) + Î²Â·m_{t-1}      [blend with previous]

where Î² âˆˆ [0, 1] is stickiness factor
```

**Effect**:
- Î² = 0: No memory of previous mode (reactive)
- Î² = 1: Never change mode (stuck)
- Î² = 0.1 (typical): Smooth transitions, inertia

### 2.5 Attention Bank (Optional Context)

**Learnable Context Vectors**:
```
Bank âˆˆ â„^(BÃ—D)    where B = bank_size

Attention over bank:
  scores = (h_t Â· Bank^T) / âˆšD
  weights = softmax(scores)    âˆˆ â„á´®
  context = weights Â· Bank     âˆˆ â„á´°

Readout input: [h_t, context]  âˆˆ â„^(2D)
```

**Purpose**: Provides additional context beyond current memory, learned during training.

### 2.6 Readout Network

**CRITICAL**: Input dimension must match concatenation:
```
If using bank:
  readout_input = [h_t, context]
  dim(readout_input) = D + D = 2D

MLP: â„^(2D) â†’ â„á´´ â†’ â„á´°_out

ERROR (original): Linear(D + bank_size, ...) assumed wrong concatenation
FIX: Linear(2D, ...) since context âˆˆ â„á´°
```

---

## 3. Invariants & Constraints

### 3.1 Mass Conservation (PMM)

**Mathematical Statement**:
```
âˆ€t: Î£áµ¢âˆˆActive occupancy_i(t) = 1.0
```

**Enforcement**:
```
After every update:
  total = Î£áµ¢ occupancy_i
  if total > Îµ:
    occupancy_i â† occupancy_i / total
  else:
    occupancy_i â† 1 / n_active    [equal distribution]
```

**Why Critical**: Violating this breaks probabilistic interpretation and can cause numerical instability.

### 3.2 Non-Negativity (PMM)

**Constraints**:
```
Î»áµ¢ â‰¥ 0    (importance)
Î³áµ¢ âˆˆ [0, 1]    (decay rate)
Ï‰áµ¢ â‰¥ 0    (frequency)
```

**Enforcement**:
```
After EMA update:
  Î»áµ¢ â† max(Î»áµ¢, Î»_min)    where Î»_min = 1e-6
  Î³áµ¢ â† clip(Î³áµ¢, 0, 1)
  Ï‰áµ¢ â† max(Ï‰áµ¢, 0)
```

### 3.3 Simplex Constraints

**PMM Occupancy**:
```
occupancy âˆˆ Î”á´·    (K-simplex)
```

**FRNN Modes**:
```
m_t âˆˆ Î”á´·    (automatically satisfied by softmax)
```

**Feelings**:
```
F_t âˆˆ Î”â¸    (8-simplex for emotions)

Enforcement: F â† F / sum(F) after each update
```

### 3.4 Capacity Constraints

```
n_active_modes â‰¤ max_modes

Enforcement:
  - Prune underutilized modes if at capacity
  - Block splits if at capacity
```

---

## 4. Gradient Flow Analysis

### 4.1 Parameters vs Buffers

**Parameters** (receive gradients):
```
PMM:
  - Î¼áµ¢ (prototypes)
  - w (weights - if used)

FRNN:
  - All MLP weights
  - Bank vectors

Tonenet:
  - Projection matrix
```

**Buffers** (no gradients, manual updates):
```
PMM:
  - Î»áµ¢ (importance) - updated via EMA
  - Î³áµ¢, Ï‰áµ¢, Ï†áµ¢ (spectral) - updated via EMA
  - occupancy - updated via EMA
  - active_mask - boolean, no gradients needed
```

**Why Separation?**:
- Prototypes (Î¼) trained via backprop
- Dynamics (Î», occupancy) updated online via statistics
- Prevents gradient interference with structural operations

### 4.2 Loss Gradients

**PMM Loss**:
```
L_total = L_recon + Î²Â·L_sparse

âˆ‚L/âˆ‚Î¼â±¼ = âˆ‚L_recon/âˆ‚Î¼â±¼ + Î²Â·âˆ‚L_sparse/âˆ‚Î¼â±¼

Components:
  âˆ‚L_recon/âˆ‚Î¼â±¼ = -2Â·Î±â±¼Â·(x - xÌ‚)
  
  âˆ‚L_sparse/âˆ‚Î¼â±¼ involves âˆ‚Î±â‚–/âˆ‚Î¼â±¼ via chain rule through softmax
```

**FRNN Loss** (task-specific):
```
L_task = task_loss(readout(x_t), target)

Gradients flow:
  L â†’ readout â†’ memory_update â†’ mode_selection â†’ input
```

### 4.3 Gradient Clipping

**Recommended**:
```
torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)

Prevents:
  - Exploding gradients
  - Catastrophic forgetting
  - Unstable mode switches
```

---

## 5. Convergence Properties

### 5.1 PMM Convergence

**Theorem** (informal): Under EMA updates with normalization, occupancy converges to a stationary distribution.

**Proof Sketch**:
```
occupancy_i^(t+1) = ÏÂ·occupancy_i^(t) + (1-Ï)Â·Î±áµ¢

At equilibrium: occupancy_i^* = Î±áµ¢^*

The normalization step projects onto simplex, ensuring:
  Î£áµ¢ occupancy_i^* = 1
```

**Convergence Rate**: O(log(1/Îµ) / (1-Ï)) steps to reach Îµ-neighborhood.

### 5.2 Mode Stability

**Stable Configuration**:
```
- No two modes too similar (no merge triggers)
- No mode overutilized with low quality (no split triggers)
- No mode underutilized (no prune triggers)
```

**Attracting Set**: System evolves toward configurations where prototypes are:
1. Well-separated
2. Balanced occupancy
3. Good reconstruction quality

### 5.3 FRNN Memory Convergence

**Theorem**: Under fixed mode distribution m_t, memory banks converge exponentially.

**Proof**:
```
M_t[k] = Ï^tÂ·M_0[k] + (1-Ï)Â·Î£_{s=0}^{t-1} Ï^sÂ·m_s[k]Â·Î”m_s

As t â†’ âˆ:
  M_âˆ[k] âˆ time-average of {m_s[k]Â·Î”m_s}
```

**Convergence Rate**: Ï„ = -1/log(Ï) steps (half-life).

For Ï = 0.99: Ï„ â‰ˆ 69 steps.

---

## 6. Information Theory Perspective

### 6.1 Entropy of Modes

**Mode Distribution Entropy**:
```
H(m_t) = -Î£â‚– m_t[k]Â·log(m_t[k])

Min: H = 0 (deterministic, one mode active)
Max: H = log(K) (uniform over K modes)
```

**Interpretation**: Measures uncertainty in mode selection.

### 6.2 Mutual Information

**Between Input and Mode**:
```
I(X; M) = H(M) - H(M|X)

High I(X; M) â†’ Input strongly determines mode
Low I(X; M) â†’ Mode selection independent of input
```

**Goal**: Learn mode selection that captures input structure.

### 6.3 Rate-Distortion Trade-off

**Compression**: PMM compresses input x âˆˆ â„á´° to mode index i âˆˆ [1, K].

**Rate**: logâ‚‚(K) bits

**Distortion**: ğ”¼[â€–x - xÌ‚â€–Â²]

**Trade-off**: More modes (higher rate) â†’ Lower distortion.

---

## 7. Comparison to Other Architectures

### 7.1 vs Transformers

| Property | Transformer | FRNN |
|----------|-------------|------|
| Attention | O(TÂ²) | O(K) discrete modes |
| Memory | Positional encoding | Explicit memory banks |
| Interpretability | Attention maps | Discrete mode probs |
| Scalability | Quadratic | Linear in modes |

### 7.2 vs Standard RNN

| Property | RNN | FRNN |
|----------|-----|------|
| State | Continuous h_t âˆˆ â„á´° | Discrete m_t âˆˆ Î”á´· |
| Capacity | Limited by hidden dim | K independent memories |
| Interpretability | Opaque | Clear mode semantics |

### 7.3 vs Vector Quantization (VQ-VAE)

| Property | VQ-VAE | PMM |
|----------|--------|-----|
| Codebook | Fixed K entries | Dynamic merge/split |
| Update | Hard assignment | Soft (differentiable) |
| Online | No | Yes (EMA updates) |

---

## Summary of Key Equations

**PMM Reconstruction**:
```
Î±áµ¢ = softmax(cos_sim(x, Î¼áµ¢) / Ï„)
xÌ‚ = Î£áµ¢ Î±áµ¢ Â· Î¼áµ¢
```

**PMM Dynamics**:
```
occupancy_i â† ÏÂ·occ + (1-Ï)Â·mean(Î±áµ¢)
occupancy â† occupancy / sum(occupancy)
```

**FRNN Mode**:
```
m_t = gumbel_softmax(MLP(x_t))
m_t â† (1-Î²)Â·m_t + Î²Â·m_{t-1}
```

**FRNN Memory**:
```
h_t = Î£â‚– m_t[k] Â· M_t[k]
M_{t+1}[k] = ÏÂ·M_t[k] + (1-Ï)Â·m_t[k]Â·Î”m_t
```

**Feelings**:
```
F_{t+1} = Î±Â·one_hot(tone) + (1-Î±)Â·F_t
F â† F / sum(F)
```

---

**For implementation details, see `CAPSULE_BRAIN_BUILD_GUIDE.md`**
